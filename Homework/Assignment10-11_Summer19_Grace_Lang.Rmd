---
title: "Assignment10-11_Summer19_Grace_Lang"
author: "Grace Lang"
date: "7/17/2019"
output: html_document
---

# A1-5) Clean and Prepare the data:
```{r setup}

library(dplyr)
library(tidyverse)
library(ggplot2)
library(stats)
library(caret)
library(rmarkdown)
library(class)
library(mltools)

#Loading the two files
BeersCSV <- "https://raw.githubusercontent.com/BivinSadler/MSDS-6306-Doing-Data-Science/master/Unit%207/Beers.csv"
Beers <- read.csv(url(BeersCSV))

BrewCSV <- "https://raw.githubusercontent.com/BivinSadler/MSDS-6306-Doing-Data-Science/master/Unit%207/Breweries.csv"
Brew <- read.csv(url(BrewCSV))

#Merging the two files
colnames(Beers)[colnames(Beers)=="Brewery_id"] <- "Brew_ID" #Rename for the same key
Combined <- merge(Beers, Brew, by = "Brew_ID") # Merge files
colnames(Combined)[colnames(Combined)=="Name.x"] <- "Beer Name"
colnames(Combined)[colnames(Combined)=="Name.y"] <- "Brewery_Name"

##Check to see where spaces fall
#head(paste(Combined$State),5)

#Create a function for trimming states
trim <- function(x) { gsub("(^[[:space:]]+|[[:space:]]+$)", "", x) }
Combined_Trimmed <- Combined[,1:9]
Trimmed_State <- data.frame(State = trim(Combined$State))
Combined <- cbind(Combined_Trimmed,Trimmed_State)

##Check to ensure spaces are gone
#head(paste(Combined$State),5)

#One dataset with no IBU/NAs and only filter on TX & CO
beerCOTX <- Combined %>% subset.data.frame(State == c("CO","TX")) %>% arrange(IBU) %>% na.omit()

```

# B8-12 External Cross Validation
Create a test training dataset for Texas & print summaries
```{r ECV}
beerTX <- subset.data.frame(beerCOTX, State == "TX")
set.seed(7)
TrainTX = sample(seq(1,dim(beerTX)[1]),round(.60*dim(beerTX)[1]),replace = FALSE)
#TX_Train
Training_TX = beerTX[TrainTX,]
#TX_Test
Test_TX = beerTX[-TrainTX,]

#Check to see if it pulled 60/40% 
#dim(Training_TX) 27 10 
#dim(Test_TX) 18 10

summary(Training_TX)
summary(Test_TX)
```    

Using the training data, fit a KNN regression model to predict ABV from IBU.  You should use the knnreg function in the caret package.  Fit two separate models: one with k = 3 and one with k = 5.  (This is 2 models total.)
```{r knn}
#KNN3 <- knnreg(IBU~ABV, data = Training_TX, k=3)
#summary(KNN3)
#KNN3
#plot(Test_TX$ABV ,predict(KNN3,Test_TX$IBU))
#model1 <- knn(train = Training_TX, test = Test_TX, cl= Training_TX$IBU, k=30)
#fit <- knnreg(Training_TX$IBU, Training_TX$ABV, k=3)
#plot(Test_TX$ABV ,predict(fit,Test_TX$IBU))
#KNN3 <- knnregTrain(Training_TX[,c(4:5)], Test_TX[,c(4:5)], Training_TX$IBU, k=3)
#predict(KNN3, Test_TX)
##plot(Test_TX ,predict(KNN3,Test_TX))

#Setting up two KNN reg 
set.seed(7)
plot(Training_TX$IBU, Training_TX$ABV, main = "KNN frame")
knn3 <- knnreg(ABV~IBU, data = Training_TX, k=3)
knn5 <- knnreg(ABV~IBU, data = Training_TX, k=5)

predict3 <- predict(knn3, Test_TX)
predict5 <- predict(knn5, Test_TX)


```
Use the ASE loss function and external cross validation to provide evidence as to which model (k = 3 or k = 5) is more appropriate.
```{r ASE}
ase3 <- mse(predict3, Test_TX$ABV)
ase5 <- mse(predict5, Test_TX$ABV)

ase3
ase5
```
In this example the k = 5 model has the better fit because of the lower ASE of 4.172047e-05 versus the k = 3 model which had a result of 5.225094e-05. 


Creating an automatic KNN that runs multiple versions to see if there's anything different then the basic knn function. 
```{r automated version of B9}
set.seed(7)
#Setting the number of times we will train our KNN
trainMethod <- trainControl(method = "repeatedcv",
                            number = 10,
                            repeats = 3)

#training multiple KNNs to see which K has the best fit
knn_fit <- train(ABV~IBU, 
                 data = Training_TX, 
                 method = "knn",
                 trControl = trainMethod,
                 preProcess = c("center", "scale"),
                 tuneLength = 10)
knn_fit
plot(knn_fit)

#predict using test dataset
predict_test <- predict(knn_fit, newdata = Test_TX)
predict_test

```    
Now use the ASE loss function and external cross validation to provide evidence as to which model (the linear regression model from last week or the “best” KNN regression model from this week (from question 10)) is more appropriate. 


Use your “best” KNN regression model to predict the ABV for an IBU of 150, 170 and 190.  What issue do you see with using KNN to extrapolate?    
```{r predicting values}
PredictValues <- data.frame(IBU = c(150,170,190))

PredictValues$ABV_Predict <- predict(knn5, PredictValues)
PredictValues

```
The issue that arises when using KNN to predict ABV is that the predictions are all identical. 

## KNN Classification (13 - 16)
Filter the beerCOTX dataframe for only beers that are from Texas and are American IPA and American Pale Ale. 
Divide this filtered data set into a training and test set (60/40, training / test split).
```{r filter data}
APA <- subset.data.frame(beerTX, Style == "American Pale Ale (APA)")
IPA <- subset.data.frame(beerTX, Style == "American IPA")
beerClass <- rbind.data.frame(APA, IPA)

set.seed(7)
Class = sample(seq(1,dim(beerClass)[1]),round(.60*dim(beerClass)[1]),replace = FALSE)
#TX_Train
Train_Beer = beerClass[Class,]
#TX_Test
Test_Beer = beerClass[-Class,]

#Check to see if it pulled 60/40
##dim(Test_Beer) 3 10 
##dim(Train_Beer) 5 10 

```
Use the class package’s knn function to build an KNN classifier with k = 3 that will use ABV and IBU as features (explanatory variables) to classify Texas beers as American IPA or American Pale Ale using the Training data.  Use your test set to create a confusion table to estimate the accuracy, sensitivity and specificity of the model.

```{r knn class}
library(e1071)

results <- class::knn(Train_Beer[,c(4:5)], Test_Beer[,c(4:5)], Train_Beer$Style, k=3)
Test_Beer$ClassPredict <- results

confusionMatrix(
  factor(Test_Beer$Style, levels = c("American Pale Ale (APA)","American IPA")),
  factor(Test_Beer$ClassPredict, levels = c("American Pale Ale (APA)","American IPA"))
)

```

```{r knn 5 class}
results_5 <- class::knn(Train_Beer[,c(4:5)], Test_Beer[,c(4:5)], Train_Beer$Style, k=5)
Test_Beer$ClassPredict_5 <- results_5

confusionMatrix(
  factor(Test_Beer$Style, levels = c("American Pale Ale (APA)","American IPA")),
  factor(Test_Beer$ClassPredict_5, levels = c("American Pale Ale (APA)","American IPA"))
)

```
The K = 3 model classifier was more accurate than the K = 5 model with a 1 on accuracy. The sensitivity is 100%, but it had a specificity of 100% also. Because of this large room for error, it may be beneficial to run this same model on a larger Test data set (maybe not just the Texas Beers).

# Unit 11 Questions
## 

```{r NYT data}
library(keyring)

NYTIMES_KEY = "xx " #Your Key Here … get from NTY API website
key_set("MY_FAKE_TOKEN")


# Let's set some parameters
term <- "Central+Park+Jogger" # Need to use + to string together separate words
begin_date <- "19890419"
end_date <- "19910419" #how do we know the structure of this field is YYYYMMDD ?

baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
                  "&begin_date=",begin_date,"&end_date=",end_date,
                  "&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
#used to concat ful URL for search
baseurl

initialQuery <- jsonlite::fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1) #whats with the multiple $ signs
maxPages

pages <- list()
for(i in 0:maxPages){
  nytSearch <- jsonlite::fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame() 
  message("Retrieving page ", i)
  pages[[i+1]] <- nytSearch 
  Sys.sleep(7) 
}

allNYTSearch <- rbind_pages(pages)

